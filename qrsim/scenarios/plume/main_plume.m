% bare bones example of use of the QRSim() simulator
% with the plume scenario
%
% in this scenario one or more plumes (that might evolve over time) are present in the flight area
% an helicopter agent is equipped with a sensor that measures the concentration of smoke.
% The plume follows a known model but with unknown parameter values.
% The objective is to provide a smoke concentration estimate cT at some prespecified time T


% REMINDER:
% to turn of visualization set the task parameter taskparams.display3d.on to 0

clear all
close all

% include simulator
addpath(['..',filesep,'..',filesep,'sim']);
addpath(['..',filesep,'..',filesep,'controllers']);

% create simulator object
qrsim = QRSim();

% load task parameters and do housekeeping
state = qrsim.init('TaskPlumeSingleSourceGaussian');
%state = qrsim.init('TaskPlumeSingleSourceGaussianDispersion');
%state = qrsim.init('TaskPlumeMultiSourceGaussianDispersion');
%state = qrsim.init('TaskPlumeMultiHeliMultiSourceGaussianDispersion');
%state = qrsim.init('TaskPlumeSingleSourceGaussianPuffDispersion');
%state = qrsim.init('TaskPlumeMultiSourceGaussianPuffDispersion');
%state = qrsim.init('TaskPlumeMultiHeliMultiSourceGaussianPuffDispersion');


% create a 3 x helicopters matrix of control inputs
% column i will contain the 3D NED velocity [vx;vy;vz] in m/s for helicopter i
U = zeros(3,state.task.numUAVs);
tstart = tic;

% allocate up a matrix to store all the concentration measuremenst
plumeMeas = zeros(state.task.numUAVs,state.task.durationInSteps);

% get from the task the list of location the agent needs to return
% precdictions at.
positions = state.task.getLocations();

% numbero of predictions the agent needs to return for each location,
% this will be > 1 only if the concentration is time variant i.e. a puff
% model
samplesPerLocation = state.task.getSamplesPerLocation();


% allocate temporary array for control inputs
u = zeros(2,state.task.numUAVs);

% run the scenario and at every timestep generate a control
% input for each of the uavs
% note: the duration of the task might need changing depending
% on the way the learning is carried out
for i=1:state.task.durationInSteps,
    tloop=tic; 
    
    % a basic policy in which the helicopter(s) moves around
    % at a fix velocity changing direction every once in a while    
    % to make our life easier vz is always 0.
    if(rem(i-1,10)==0)
        for j=1:state.task.numUAVs,
            
            % one should alway make sure that the uav is valid 
            % i.e. no collision or out of area event happened
            if(state.platforms{j}.isValid())                
                % random velocity direction
                u(:,j) = rand(2,1)-[0.5;0.5];
            
                % scale by the max allowed velocity
                U(:,j) = [0.5*state.task.velPIDs{j}.maxv*(u(:,j)/norm(u(:,j)));0];
                                
                % if the uav is going astray we point it back to the center
                p = state.platforms{j}.getEX(1:2);
                if(norm(p)>100)
                    U(:,j) = [-0.8*state.task.velPIDs{j}.maxv*p/norm(p);0];
                end    
            end
        end
    end
    
    % step simulator
    qrsim.step(U);

    % get plume measurement
    for j=1:state.task.numUAVs,
        plumeMeas(j,i)=state.platforms{j}.getPlumeSensorOutput();
    end
    
    if(state.display3dOn)
        % wait so to run in real time
        % this can be commented out obviously
        wait = max(0,state.task.dt-toc(tloop));
        pause(wait);
    end
end

% allocate a matrix to store all the predictions made by the agent
samples = zeros(samplesPerLocation,size(positions,2));

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% for the purpouse of demonstration, we CHEAT and use samples
%%% generated by the environment itself (i.e. perfect estimations)
samples = state.environment.area.getReferenceSamples();
%%% in a real experiment getReferenceSamples should not be used and
%%% the samples should come from the and model that the agent has
%%% somehow estimated
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% pass the sample to the task so that a reward can compute a reward
state.task.setSamples(samples);

% get final reward (mnote: this works only after the samples have been set!).
% reminder: a large negative final reward (-1000) is returned in case of
% collisions or in case of any uav going outside the flight area
fprintf('final reward: %f\n',qrsim.reward());

elapsed = toc(tstart);

fprintf('running %d times real time\n',(state.task.durationInSteps*state.task.dt)/elapsed);
